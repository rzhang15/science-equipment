# 5_validate_utdallas.py (Streamlined for Validation ONLY)
"""
Validates the output of 3_predict_product_markets.py against the
UT Dallas ground truth labels.

This script does NOT re-run any models. It READS the predictions
from the CSV file generated by Script 3.

Uses key-based merging (not positional alignment) to safely match
predictions to ground truth.

ASSUMPTION: You have already run 3_predict_product_markets.py on
the 'utdallas' source.
"""
import pandas as pd
import os
import argparse
from sklearn.metrics import classification_report

import config

def main(gatekeeper_name: str, expert_choice: str, min_support: int = 25):
    print("="*80)
    print("--- Starting Validation on PRE-CLASSIFIED UT Dallas Data ---")
    print(f"  - Gatekeeper Model:      {gatekeeper_name}")
    print(f"  - Expert Model Choice:   {expert_choice}")
    print(f"  - Summary Report Support: >= {min_support} items")

    # 1. Define the input file path from Script 3
    classified_filename = f"utdallas_merged_clean_classified_with_{expert_choice}.csv"
    classified_filepath = os.path.join(config.OUTPUT_DIR, classified_filename)

    try:
        # 2. Load the PREDICTIONS from Script 3's output
        print(f"\nLoading predictions from: {classified_filepath}")
        df_classified = pd.read_csv(classified_filepath)
    except FileNotFoundError:
        print(f"FATAL ERROR: The prediction file was not found.")
        print(f"   Expected file: {classified_filepath}")
        print(f"   Please run 3_predict_product_markets.py on the 'utdallas' source first.")
        return
    except KeyError:
        print(f"FATAL ERROR: The file '{classified_filename}' is missing required columns.")
        return

    try:
        # 3. Load the TRUE LABELS from the original UT Dallas file
        print(f"Loading true labels from: {config.UT_DALLAS_MERGED_CLEAN_PATH}")
        df_validation = pd.read_parquet(config.UT_DALLAS_MERGED_CLEAN_PATH)
    except FileNotFoundError:
        print(f"FATAL ERROR: The original UT Dallas data file was not found.")
        print(f"   Expected file: {config.UT_DALLAS_MERGED_CLEAN_PATH}")
        return

    # --- Positional alignment ---
    # Script 3 reads the same parquet file and writes one prediction row per input
    # row in the same order, so positional alignment is correct and avoids the
    # many-to-many merge bug caused by duplicate merge keys.
    if len(df_classified) != len(df_validation):
        print(f"FATAL ERROR: Row count mismatch ({len(df_classified)} vs {len(df_validation)}).")
        print("  The classified file must have the same rows in the same order as the truth file.")
        return

    print(f"  - Aligned {len(df_classified)} prediction rows to {len(df_validation)} ground truth rows (positional).")
    y_pred = df_classified['predicted_market'].fillna("unclassified")
    y_true_full = df_validation[config.UT_CAT_COL]

    # 4. Simplify true labels (needed for fair comparison)
    print("Simplifying true labels to match 'Non-Lab' category...")
    is_true_nonlab = y_true_full.str.contains(config.NONLAB_REGEX, na=False)
    y_true_simplified = y_true_full.copy()
    y_true_simplified.loc[is_true_nonlab] = 'Non-Lab'
    y_true_simplified = y_true_simplified.fillna("unclassified")

    # 5. Generate classification report
    print("\nGenerating full performance report...")
    report_dict = classification_report(y_true_simplified, y_pred, zero_division=0, output_dict=True)
    df_report = pd.DataFrame(report_dict).transpose()
    df_report['support'] = df_report['support'].astype(int)

    # 6. Generate and save the output REPORT files
    print("Generating and saving report files...")

    base_name_report = f"gatekeeper_{gatekeeper_name}_expert_{expert_choice}"

    path_full_report_csv = os.path.join(config.OUTPUT_DIR, f"utdallas_full_report_{base_name_report}.csv")
    path_summary_report_txt = os.path.join(config.OUTPUT_DIR, f"utdallas_summary_report_{base_name_report}.txt")

    df_report.to_csv(path_full_report_csv)
    print(f"  - Full performance report saved to: {path_full_report_csv}")

    # Create the summary text report
    df_summary = df_report[df_report['support'] >= min_support]
    with open(path_summary_report_txt, 'w') as f:
        f.write(f"Summary Performance Report on FULL DATASET (Support >= {min_support})\n")
        f.write(f"Based on predictions from: {classified_filename}\n")
        f.write(f"Gatekeeper: {gatekeeper_name}, Expert: {expert_choice}\n")
        f.write("="*70 + "\n")
        f.write(df_summary.to_string())
    print(f"  - Summary report saved to: {path_summary_report_txt}")
    print("\n--- Validation Complete ---")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Validate PRE-CLASSIFIED pipeline output on the FULL UT Dallas dataset."
    )
    parser.add_argument(
        "--gatekeeper",
        type=str,
        required=True,
        choices=['tfidf', 'bert'],
        help="The gatekeeper name used to generate the input file."
    )
    parser.add_argument(
        "--expert",
        type=str,
        required=True,
        choices=['non_parametric_tfidf', 'non_parametric_bert'],
        help="The expert model name used to generate the input file."
    )
    parser.add_argument(
        "--min_support",
        type=int,
        default=25,
        help="Minimum support for a category to appear in the summary text report."
    )
    args = parser.parse_args()
    main(gatekeeper_name=args.gatekeeper, expert_choice=args.expert, min_support=args.min_support)
