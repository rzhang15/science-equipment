# 5_validate_utdallas.py (Streamlined for Validation ONLY)
"""
Validates the output of 3_predict_product_markets.py against the 
UT Dallas ground truth labels.

This script does NOT re-run any models. It READS the predictions
from the CSV file generated by Script 3.

ASSUMPTION: You have already run 3_predict_product_markets.py on
the 'utdallas' source.
"""
import pandas as pd
import os
import argparse
from sklearn.metrics import classification_report

import config

def main(gatekeeper_name: str, expert_choice: str, min_support: int = 25):
    print("="*80)
    print("--- Starting Validation on PRE-CLASSIFIED UT Dallas Data ---")
    print(f"  - This script will validate the output file for:")
    print(f"  - Gatekeeper Model:      {gatekeeper_name}")
    print(f"  - Expert Model Choice:   {expert_choice}")
    print(f"  - Summary Report Support: >= {min_support} items")

    # 1. Define the input file path from Script 3
    # This MUST match the output format of 3_predict_product_markets.py
    # when run on 'utdallas_merged_clean.parquet'
    classified_filename = f"utdallas_merged_clean_classified_with_{expert_choice}.csv"
    classified_filepath = os.path.join(config.OUTPUT_DIR, classified_filename)

    try:
        # 2. Load the PREDICTIONS from Script 3's output
        print(f"\nℹ️ Loading predictions from: {classified_filepath}")
        df_classified = pd.read_csv(classified_filepath)
        # Handle potential empty descriptions from Script 3
        y_pred = df_classified['predicted_market'].fillna("unclassified") 
    
    except FileNotFoundError:
        print(f"❌ FATAL ERROR: The prediction file was not found.")
        print(f"   Expected file: {classified_filepath}")
        print(f"   Please run 3_predict_product_markets.py on the 'utdallas' source first.")
        return
    except KeyError:
        print(f"❌ FATAL ERROR: The file '{classified_filename}' is missing the 'predicted_market' column.")
        return

    try:
        # 3. Load the TRUE LABELS from the original UT Dallas file
        print(f"ℹ️ Loading true labels from: {config.UT_DALLAS_MERGED_CLEAN_PATH}")
        df_validation = pd.read_parquet(config.UT_DALLAS_MERGED_CLEAN_PATH)
        y_true_full = df_validation[config.UT_CAT_COL]
    
    except FileNotFoundError:
        print(f"❌ FATAL ERROR: The original UT Dallas data file was not found.")
        print(f"   Expected file: {config.UT_DALLAS_MERGED_CLEAN_PATH}")
        return
        
    # --- Check for length mismatch ---
    if len(y_pred) != len(y_true_full):
        print(f"❌ FATAL ERROR: Mismatch in row counts.")
        print(f"   Predictions file has {len(y_pred)} rows.")
        print(f"   True labels file has {len(y_true_full)} rows.")
        print(f"   Please re-run Script 3 on the correct UT Dallas file.")
        return

    # 4. Simplify true labels (This logic is still needed for a fair comparison)
    print("ℹ️ Simplifying true labels to match 'Non-Lab' category...")
    nonlab_pattern = '|'.join(config.NONLAB_CATEGORIES)
    is_true_nonlab = y_true_full.str.contains(nonlab_pattern, case=False, na=False)
    y_true_simplified = y_true_full.copy()
    y_true_simplified.loc[is_true_nonlab] = 'Non-Lab'
    y_true_simplified = y_true_simplified.fillna("unclassified")

    # 5. Generate classification report
    print("\nℹ️ Generating full performance report...")
    report_dict = classification_report(y_true_simplified, y_pred, zero_division=0, output_dict=True)
    df_report = pd.DataFrame(report_dict).transpose()
    df_report['support'] = df_report['support'].astype(int)

    # 6. Generate and save the output REPORT files
    print("✅ Generating and saving report files...")
    
    base_name_report = f"gatekeeper_{gatekeeper_name}_expert_{expert_choice}"
    
    path_full_report_csv = os.path.join(config.OUTPUT_DIR, f"utdallas_full_report_{base_name_report}.csv")
    path_summary_report_txt = os.path.join(config.OUTPUT_DIR, f"utdallas_summary_report_{base_name_report}.txt")

    df_report.to_csv(path_full_report_csv)
    print(f"  - Full performance report saved to: {path_full_report_csv}")
    
    # Create the summary text report
    df_summary = df_report[df_report['support'] >= min_support]
    with open(path_summary_report_txt, 'w') as f:
        f.write(f"Summary Performance Report on FULL DATASET (Support >= {min_support})\n")
        f.write(f"Based on predictions from: {classified_filename}\n")
        f.write(f"Gatekeeper: {gatekeeper_name}, Expert: {expert_choice}\n")
        f.write("="*70 + "\n")
        f.write(df_summary.to_string())
    print(f"  - Summary report saved to: {path_summary_report_txt}")
    print("\n--- Validation Complete ---")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Validate PRE-CLASSIFIED pipeline output on the FULL UT Dallas dataset."
    )
    parser.add_argument(
        "--gatekeeper", 
        type=str, 
        required=True, 
        choices=['tfidf', 'bert'],
        help="The gatekeeper name used to generate the input file."
    )
    parser.add_argument(
        "--expert", 
        type=str, 
        required=True, 
        choices=['non_parametric_tfidf', 'non_parametric_bert'],
        help="The expert model name used to generate the input file."
    )
    parser.add_argument(
        "--min_support", 
        type=int, 
        default=25, 
        help="Minimum support for a category to appear in the summary text report."
    )
    args = parser.parse_args()
    main(gatekeeper_name=args.gatekeeper, expert_choice=args.expert, min_support=args.min_support)